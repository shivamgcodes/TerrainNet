{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,models\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import LeakyReLU"
      ],
      "metadata": {
        "id": "Xv1MFsJmBWei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "8Fzqw3ahiU0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWz9TzrhkPVn",
        "outputId": "a2a865ea-eb02-424a-d054-ba47418981f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = np.load('/content/drive/MyDrive/Data/images10.npy')\n",
        "labels = np.load('/content/drive/MyDrive/Data/labels10.npy')"
      ],
      "metadata": {
        "id": "A1igs5t7GAHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "labels_one_hot = encoder.fit_transform(labels.reshape(-1, 1)).toarray()\n",
        "\n",
        "height, width, channels = images.shape[1], images.shape[2], images.shape[3]\n",
        "num_classes = labels_one_hot.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), input_shape = (height, width, channels)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(images, labels_one_hot, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=16, validation_data=(X_val, y_val))\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "model.save('terrain_recognition_model.h5')\n"
      ],
      "metadata": {
        "id": "WbGbNEN_IyFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93fb7456-7a6e-4781-defe-58eb702644e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "13/13 [==============================] - 12s 55ms/step - loss: 1.1215 - accuracy: 0.5224 - val_loss: 0.7274 - val_accuracy: 0.6860\n",
            "Epoch 2/16\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.7762 - accuracy: 0.6990 - val_loss: 0.5622 - val_accuracy: 0.7209\n",
            "Epoch 3/16\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.5819 - accuracy: 0.7587 - val_loss: 0.3857 - val_accuracy: 0.8372\n",
            "Epoch 4/16\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.4658 - accuracy: 0.8259 - val_loss: 0.4351 - val_accuracy: 0.8023\n",
            "Epoch 5/16\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.4203 - accuracy: 0.8209 - val_loss: 0.2683 - val_accuracy: 0.8605\n",
            "Epoch 6/16\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.4701 - accuracy: 0.7960 - val_loss: 0.3925 - val_accuracy: 0.8372\n",
            "Epoch 7/16\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.3677 - accuracy: 0.8532 - val_loss: 0.3792 - val_accuracy: 0.8256\n",
            "Epoch 8/16\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.3141 - accuracy: 0.8706 - val_loss: 0.2629 - val_accuracy: 0.8953\n",
            "Epoch 9/16\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.3729 - accuracy: 0.8383 - val_loss: 0.3532 - val_accuracy: 0.8488\n",
            "Epoch 10/16\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.2912 - accuracy: 0.8905 - val_loss: 0.2541 - val_accuracy: 0.8953\n",
            "Epoch 11/16\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.2541 - accuracy: 0.8955 - val_loss: 0.1931 - val_accuracy: 0.9302\n",
            "Epoch 12/16\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.2001 - accuracy: 0.9279 - val_loss: 0.2932 - val_accuracy: 0.8953\n",
            "Epoch 13/16\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.1850 - accuracy: 0.9353 - val_loss: 0.3961 - val_accuracy: 0.8372\n",
            "Epoch 14/16\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.1867 - accuracy: 0.9502 - val_loss: 0.2967 - val_accuracy: 0.8837\n",
            "Epoch 15/16\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.1434 - accuracy: 0.9453 - val_loss: 0.1357 - val_accuracy: 0.9767\n",
            "Epoch 16/16\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.1254 - accuracy: 0.9627 - val_loss: 0.1263 - val_accuracy: 0.9651\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.2748 - accuracy: 0.9080\n",
            "Test Accuracy: 90.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mahotas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7z18MBWzTNX",
        "outputId": "2c2df95b-1990-4cc5-906c-3cbfdcd79d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mahotas\n",
            "  Downloading mahotas-1.4.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mahotas) (1.23.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import mahotas.features\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load the pre-trained CNN model\n",
        "cnn_model = load_model('/content/terrain_recognition_model.h5')\n",
        "\n",
        "# Function to extract Haralick texture features (contrast, energy, entropy)\n",
        "def extract_haralick_features(image):\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate Haralick texture features\n",
        "    haralick_features = mahotas.features.haralick(gray_image)\n",
        "\n",
        "    # Calculate contrast, energy, and entropy\n",
        "    contrast = haralick_features[:, 1].mean()\n",
        "    energy = haralick_features[:, 2].mean()\n",
        "    entropy = haralick_features[:, 8].mean()\n",
        "\n",
        "    return contrast, energy, entropy\n",
        "\n",
        "# Load and preprocess the image you want to classify\n",
        "image_to_classify = cv2.imread('/content/drive/MyDrive/Data/cropped (8).jpg')\n",
        "image_to_classify =cv2.resize(image_to_classify,(64,64))\n",
        "\n",
        "\n",
        "# Extract Haralick texture features from the image\n",
        "contrast, energy, entropy = extract_haralick_features(image_to_classify)\n",
        "\n",
        "# Print the extracted features\n",
        "\n",
        "\n",
        "# Assuming the CNN model expects an input shape of (64, 64, 3)\n",
        "# You may need to adjust the preprocessing based on your specific CNN model\n",
        "cnn_input = np.expand_dims(image_to_classify, axis=0)  # Expand dimensions to match the CNN input shape\n",
        "\n",
        "# Make predictions using the CNN model\n",
        "predicted_class_probabilities = cnn_model.predict(cnn_input)\n",
        "\n",
        "# Assuming the CNN model outputs class probabilities, get the predicted class index\n",
        "predicted_class_index = np.argmax(predicted_class_probabilities)\n",
        "\n",
        "# Define a mapping of CNN model class labels to actual labels\n",
        "\n",
        "\n",
        "if(predicted_class_index>=2):\n",
        "\n",
        "  # Load the grayscale image\n",
        "  image = cv2.imread('/content/drive/MyDrive/Data/cropped (8).jpg', cv2.IMREAD_GRAYSCALE)\n",
        "  image = cv2.resize(image, (64, 64))\n",
        "  hist, bins = np.histogram(image.flatten(), 256, [0, 256])\n",
        "  hist_prob = hist.astype(float) / np.sum(hist)\n",
        "  max_variance = 0\n",
        "  threshold = 0\n",
        "  for i in range(1, 256):\n",
        "    w0 = np.sum(hist_prob[:i])  # Probability of class 0 (background)\n",
        "    w1 = np.sum(hist_prob[i:])  # Probability of class 1 (foreground)\n",
        "\n",
        "    mean0 = np.sum(hist_prob[:i] * np.arange(i)) / (w0 + 1e-10)  # Mean of class 0\n",
        "    mean1 = np.sum(hist_prob[i:] * np.arange(i, 256)) / (w1 + 1e-10)  # Mean of class 1\n",
        "\n",
        "    # Calculate between-class variance\n",
        "    variance_between = w0 * w1 * (mean0 - mean1) ** 2\n",
        "\n",
        "    # Update threshold if variance is greater\n",
        "    if variance_between > max_variance:\n",
        "        max_variance = variance_between\n",
        "        threshold = i\n",
        "    # Apply the optimal threshold to the image\n",
        "  binary_image = np.where(image >= threshold, 255, 0).astype(np.uint8)\n",
        "\n",
        "  if(threshold>136):\n",
        "        print(\"Rocky type\")\n",
        "\n",
        "\n",
        "  else:\n",
        "\n",
        "    #class_mapping_cnn={3: 'class_3'}\n",
        "    #print('Predicted Class (CNN):', class_mapping_cnn[predicted_class_index])\n",
        "    print(\"Sand type\")\n",
        "\n",
        "# Print the predicted class and its associated probability\n",
        "else:\n",
        "\n",
        "  class_mapping_cnn={0:'Grass type' ,1:'Marshy type' }\n",
        "  print('Predicted Class (CNN):', class_mapping_cnn[predicted_class_index])\n",
        "  print('Predicted Class Probability (CNN):', predicted_class_probabilities[0, predicted_class_index])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tice3lCCtxkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412e31d7-cfb5-47af-a120-68d8c296085c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 237ms/step\n",
            "Rocky type\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GmJLE5mdfeSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}